# -*- coding: utf-8 -*-
"""DQN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oxajrIj8T-fI83prKiMKlwXwGEPlyBpV
"""

import matplotlib.pyplot as plt
import gym
import numpy as np
import tensorflow as tf
import random
from collections import deque

class DQN:
    def __init__(self,state_size,action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen = 10000)
        self.lr = 0.0001
        self.gamma = 0.80   # discount rate
        self.epsilon = 0.1
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.99
        self.neuron = 30
        self.session = self.start_session()
        self.num_actions =2
        self.batch_size = 32
        self.x,self.action,self.label = self.add_placeholder()
        
    def add_placeholder(self):
      x = tf.placeholder(tf.float32,shape = (1,self.state_size))
      action = tf.placeholder(tf.float32, [1,self.action_size])  #1,2
      label = tf.placeholder(tf.float32, [1,]) 
      return x,action,label
    
        
    def build_model(self,state_size,action_size):
        
        weights = { 'W1': tf.Variable(tf.random_normal([self.state_size, self.neuron],dtype = tf.float32)),
                    'W2': tf.Variable(tf.random_normal([self.neuron,self.neuron],dtype = tf.float32)),
                    'out': tf.Variable(tf.random_normal([self.neuron, self.action_size],dtype = tf.float32))
                   }
        biases = {'b1': tf.Variable(tf.random_normal([1,self.neuron],dtype = tf.float32)),
                  'b2': tf.Variable(tf.random_normal([1,self.neuron],dtype = tf.float32)),
                  'bo': tf.Variable(tf.random_normal([1,self.action_size],dtype = tf.float32))
                 }
        

        self.x,self.action,self.label = self.add_placeholder()
        
        xW = tf.matmul(self.x,weights['W1'])
        h = tf.tanh(tf.add(xW, biases['b1']))
    
        xW = tf.matmul(h, weights['W2'])
        h = tf.tanh(tf.add(xW, biases['b2']))

        hU = tf.matmul(h,weights['out'])    
        self.out = tf.add(hU, biases['bo'])
        regu = 0.001 * (tf.reduce_sum(tf.square(weights['W1'])) + tf.reduce_sum(tf.square(weights['W2'])) + tf.reduce_sum(tf.square(weights['out'])))
        return self.out,regu
    
    
    def create_model_trainable(self,state_size,action_size):               #model definition
        weights = { 'W1': tf.Variable(tf.random_normal([self.state_size, self.neuron],dtype = tf.float32)),
                    'W2': tf.Variable(tf.random_normal([self.neuron,self.neuron],dtype = tf.float32)),
                    'out': tf.Variable(tf.random_normal([self.neuron, self.action_size],dtype = tf.float32))
                   }
        biases = {'b1': tf.Variable(tf.random_normal([1,self.neuron],dtype = tf.float32)),
                  'b2': tf.Variable(tf.random_normal([1,self.neuron],dtype = tf.float32)),
                  'bo': tf.Variable(tf.random_normal([1,self.action_size],dtype = tf.float32))
                 }
        

        self.x,self.action,self.label = self.add_placeholder()

        xW = tf.matmul(self.x,weights['W1'])
        h = tf.tanh(tf.add(xW, biases['b1']))
    
        xW = tf.matmul(h, weights['W2'])
        h = tf.tanh(tf.add(xW, biases['b2']))

        hU = tf.matmul(h,weights['out'])    
        self.out = tf.add(hU, biases['bo'])
        regu = 0.001 * (tf.reduce_sum(tf.square(weights['W1'])) + tf.reduce_sum(tf.square(weights['W2'])) + tf.reduce_sum(tf.square(weights['out'])))
        
        self.q_vals = tf.reduce_sum(tf.multiply(self.out, self.action), 1)
        self.loss = tf.reduce_sum(tf.square(self.label - self.q_vals)) + regu

        optimizer = tf.train.GradientDescentOptimizer(learning_rate = self.lr)
        self.train_op = optimizer.minimize(self.loss)
        
    def create_model_target(self,state_size,action_size):         #target model definiton
        weights = { 'W1': tf.Variable(tf.random_normal([self.state_size, self.neuron],dtype = tf.float32)),
                    'W2': tf.Variable(tf.random_normal([self.neuron,self.neuron],dtype = tf.float32)),
                    'out': tf.Variable(tf.random_normal([self.neuron, self.action_size],dtype = tf.float32))
                   }
        biases = {'b1': tf.Variable(tf.random_normal([1,self.neuron],dtype = tf.float32)),
                  'b2': tf.Variable(tf.random_normal([1,self.neuron],dtype = tf.float32)),
                  'bo': tf.Variable(tf.random_normal([1,self.action_size],dtype = tf.float32))
                 }
        
        self.x1,self.action1,self.label1 = self.add_placeholder()

        
        xW = tf.matmul(self.x1,weights['W1'])
        h = tf.tanh(tf.add(xW, biases['b1']))
    
        xW = tf.matmul(h, weights['W2'])
        h = tf.tanh(tf.add(xW, biases['b2']))

        hU = tf.matmul(h,weights['out'])    
        self.out1 = tf.add(hU, biases['bo'])
        regu = 0.001 * (tf.reduce_sum(tf.square(weights['W1'])) + tf.reduce_sum(tf.square(weights['W2'])) + tf.reduce_sum(tf.square(weights['out'])))
        self.predictions_target = self.out1

    def start_session(self):
        with tf.variable_scope("network") as scope:
            self.create_model_trainable(self.state_size,self.action_size)

        with tf.variable_scope("target") as scope:
            self.create_model_target(self.state_size,self.action_size)

        init = tf.initialize_all_variables()

        session = tf.Session()
        session.run(init)
        return session
    
    def remember(self,state,action,reward,state_next,done):
        transition={'state':state,'action':action,'reward':reward,'state_next':state_next,'is_done':done}
        self.memory.append(transition)
    
    def predict(self, observation):                         #Predicts the rewards for an input observation state from the target network.
        obs = observation.astype(np.float32)                  #shape = 1,4
        print(obs.shape)
        print(obs.dtype)
        b = np.zeros(len(observation),dtype = np.float32)    #shape = (1,)
        print("Shape of b is: "+str(b.shape))
        print(b.dtype)
        c = np.zeros([len(observation),self.action_size],dtype = np.float32)
        print(c.shape)                                   #shape = 1,2
        print(c.dtype)                            
        loss, pred_probs = self.session.run([self.loss,self.out1],
                                            feed_dict = {self.x:obs ,self.label:b,self.action:c})
        return pred_probs
    
    
    def predict_target(self,target):
        target.astype(np.float32)
        b = np.zeros(len(target),dtype = np.float32)
        c = np.zeros([len(target),self.action_size],dtype = np.float32)
        pred_probs = self.session.run([self.out1], feed_dict = {self.x1:obs ,self.label1:b,self.action1:c} )
      
    
    def train_step(self, Xs, ys, actions):                                            
        loss,_,pred_prob,q_values = self.session.run([self.loss,self.train_op,self.predictions,self.q_vals],
                                                    feed_dict = {self.x:Xs, self.label:ys,
                                                                 self.action:actions})
        
                                                                  
    def target_update_weights(self):
        weight_mats = ["W1", "b1", "W2", "b2", "U", "b3"]
        for mat_name in weight_mats:
            with tf.variable_scope("network",reuse = True):
                network_mat = tf.get_variable(mat_name)
            with tf.variable_scope("target", reuse=True):
                target_mat = tf.get_variable(mat_name)

            assign_op = target_mat.assign(network_mat)

            net_mat, tar_mat, _ = self.session.run([network_mat, target_mat, assign_op])
            
         
    def update_state(self, action, observation, new_observation, reward, done):
        transition = {'action': action,'observation': observation,'new_observation': new_observation,'reward': reward,
                  'is_done': done}
        self.memory.append(transition)
        
    
    def get_random_mini_batch(self):
        rand_idxs = random.sample(xrange(len(self.memory)), self.mini_batch_size)
        mini_batch = []
        for idx in rand_idxs:
            mini_batch.append(self.memory[idx])

        return mini_batch

    
    def act(self,state):
        if np.random.rand()<= self.epsilon:
            return random.randrange(self.action_size)
        else:
            states = np.array([state])
            act_values = self.predict(state)
            action = np.argmax(act_values) 
            return action
        
    
    def experiencereplay(self,batch_size):
        if(len(self.memory)<batch_size):
            return 0 
        batch = random.sample(memory,batch_size)
        for state,action,reward,next_state,terminal in batch:
            q_update = reward;
            if not_terminal:
                q_update = reward+gamma * np.argmax(self.out)
            q_values = self.out
            q_values[0][action] = q_update
        
    def train_step():                                         #updates model 
        if len(self.memory) > self.mini_batch_size:
             mini_batch = self.get_random_mini_batch()
        xs = []
        ys = []
        actions = []
        
        for sample in mini_batch:
            y = sample ['reward']
            if not sample is done:
                new_observation = sample['new_observation']
                new_obs = np.array([new_observation])
                q_new_values = self.predict(new_obs)
                action = np.max(q_new_values)
                y += self.gamma*action                                  #target
      
            action = np.zeros(self.num_actions)
            action[sample['action']] = 1
            observation = sample['observation']

            xs.append(observation.copy())
            ys.append(y)
            actions.append(action.copy())
                
        Xs = np.array(xs)
        ys = np.array(ys)
        actions = np.array(actions)

        self.model.train_step(Xs, ys, actions)
    
    def update_target(self):
    
        self.model.target_update_weights()
                                                 # Updates the target network with weights from the trained network.

env = gym.make('CartPole-v0')
observation_space = env.observation_space.shape[0];
action_space = env.action_space.n
done = False
episode = 2000
steps = 500
score = []
steps_to_update = 2000
agent = DQN(observation_space,action_space)
a = []
rew100 = deque(maxlen =100)
for i in range(episode):
    run = 0
    state = env.reset()
    state = np.reshape(state,[1,observation_space])
    rew_sum = 0
    re = []
    total_steps = 0
    for step in range(2000):
        total_steps = total_steps+1                                  
        action = agent.act(state)
        state_next,reward,done,_ = env.step(action)
        rew_sum = rew_sum + reward
        state_next = np.reshape(state_next,[1,observation_space])
        agent.remember(state,action,reward,state_next,done)
        state = state_next
        
        if done:
            print("Episode:{},Timesteps:{}, Rewards of episode:{}".format(episode,total_steps+1,rew_sum))
            rew100.append(rew_sum)
            c = np.mean(rew_100)
            print("Average reward for last 100 episodes:{}",c)
            break

        if (total_steps % steps_to_update == 0):
            print ("updating target network...")
            DQN.update_target()

        

    if (len(memory)>agent.batch_size):
        agent.experiencereplay(agent.batch_size)
    summary.value.add(tag="episode length", simple_value=episode_length)
    summary_writer.add_summary(summary, i)

3

